import numpy as np
import pandas as pd
import time
import psutil
import os
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score
)

# Step 1: Data Preprocessing 
def preprocess_data(df, label_column):
    X = df.drop(label_column, axis=1)
    y = df[label_column]

    X = pd.get_dummies(X)

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    imputer = SimpleImputer(strategy='mean')
    X_clean = imputer.fit_transform(X_scaled)

    return X_clean, y

# Step 2: Feature Selection using Simplified ET-PBO 
def evaluate_fitness(X, y, subset):
    X_sub = X[:, subset]
    X_train, X_val, y_train, y_val = train_test_split(X_sub, y, test_size=0.2, random_state=42)
    model = AdaBoostClassifier()
    model.fit(X_train, y_train)
    return accuracy_score(y_val, model.predict(X_val))

def et_pbo(X, y, num_features, pop_size=10, max_iter=10):
    np.random.seed(42)
    population = [np.random.choice(range(X.shape[1]), num_features, replace=False) for _ in range(pop_size)]

    for _ in range(max_iter):
        fitness_scores = [evaluate_fitness(X, y, subset) for subset in population]
        elite_idx = np.argmax(fitness_scores)
        best_subset = population[elite_idx]

        new_population = []
        for subset in population:
            mutated = np.copy(subset)
            idx_to_change = np.random.randint(0, len(subset))
            mutated[idx_to_change] = np.random.randint(0, X.shape[1])
            new_population.append(mutated)

        population = new_population
        population[0] = best_subset  

    return best_subset

# Step 3: Train Base Classifiers 
def train_base_classifiers(X_train, y_train):
    models = {
        'ADA': AdaBoostClassifier(),
        'QDA': QuadraticDiscriminantAnalysis(),
        'RF': RandomForestClassifier()
    }
    for name in models:
        models[name].fit(X_train, y_train)
    return models

# Step 4: Get Meta Features  
def get_meta_features(models, X):
    return np.column_stack([model.predict(X) for model in models.values()])

def train_meta_classifier(meta_features, y):
    meta_clf = LogisticRegression()
    meta_clf.fit(meta_features, y)
    return meta_clf

#  Step 5: Final Prediction 
def predict(models, meta_clf, X_test):
    meta_test_features = get_meta_features(models, X_test)
    return meta_clf.predict(meta_test_features)


def run_pipeline(df, label_column):
    # Step 1
    X, y = preprocess_data(df, label_column)

    # Step 2
    selected_features = et_pbo(X, y, num_features=5)
    X_selected = X[:, selected_features]

    # Train/Val/Test Split
    X_train, X_temp, y_train, y_temp = train_test_split(X_selected, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

    # Step 3
    models = train_base_classifiers(X_train, y_train)

    # Step 4
    meta_features_val = get_meta_features(models, X_val)
    meta_clf = train_meta_classifier(meta_features_val, y_val)

    # Step 5
    start_time = time.time()
    cpu_start = psutil.cpu_percent(interval=None)

    y_pred = predict(models, meta_clf, X_test)

    cpu_end = psutil.cpu_percent(interval=None)
    end_time = time.time()

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    kappa = cohen_kappa_score(y_test, y_pred)

    total_time = end_time - start_time
    latency = total_time / len(X_test)
    throughput = len(X_test) / total_time
    cpu_util = (cpu_start + cpu_end) / 2

    print("\n----- Evaluation Metrics -----")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Kappa Score: {kappa:.4f}")
    print(f"Throughput (pred/sec): {throughput:.2f}")
    print(f"Latency (sec/pred): {latency:.6f}")
    print(f"CPU Utilization (%): {cpu_util:.2f}")

    return y_pred

# ==== Sample CSV Generator ==== #
def generate_sample_csv(file_path="sample_data.csv"):
    np.random.seed(42)
    data = {
        "Feature1": np.random.rand(200),
        "Feature2": np.random.rand(200),
        "Feature3": np.random.randint(0, 5, 200),
        "Feature4": np.random.choice(['A', 'B', 'C'], 200),
        "Feature5": np.random.rand(200),
        "Label": np.random.choice([0, 1], 200)
    }
    df = pd.DataFrame(data)
    df.to_csv(file_path, index=False)
    print(f"âœ… Sample CSV generated: {file_path}")
    return file_path

# ==== Run Example ==== #
if __name__ == "__main__":
    csv_path = r"your_datset_path.csv"
    if not os.path.exists(csv_path):
        generate_sample_csv(csv_path)

    df = pd.read_csv(csv_path)
    predicted_labels = run_pipeline(df, label_column='your_target_label')
